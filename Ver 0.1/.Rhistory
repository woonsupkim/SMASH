cat2
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'C:/Users/Woon/Desktop/Columbia/Applied Analytics/Term2/APAN5205/Project')
cc_number = function()# Get the credit card number
{
params$cc_number
}
cc_number=cc_number()
cc_number
library(dplyr)
library(skimr)
library(ggplot2)
library(lubridate)
data = read.csv('clean_data.csv')
data = data %>% mutate(across(where(is.character),as.factor))
data$cc_num <- as.factor(data$cc_num)
data$zip <- as.factor(data$zip)
data$trans_date_trans_time = as_datetime(data$trans_date_trans_time)
data_bucket = data %>% group_by(cc_num) %>% summarize('count' = n())
library(cluster)
silhoette_width = sapply(2:20,
FUN = function(x) pam(x=data_bucket$count, k=x)$silinfo$avg.width)
ggplot(data=data.frame(cluster = 2:20,silhoette_width), aes(x=cluster,y=silhoette_width))+
geom_line(col='steelblue',size=1.2)+
geom_point()+
scale_x_continuous(breaks=seq(2,20,1))
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'C:/Users/Woon/Desktop/Columbia/Applied Analytics/Term2/APAN5205/Project')
cc_number = function()# Get the credit card number
{
params$cc_number
}
cc_number=cc_number()
cc_number
library(dplyr)
library(skimr)
library(ggplot2)
library(lubridate)
data = read.csv('clean_data.csv')
data = data %>% mutate(across(where(is.character),as.factor))
data$cc_num <- as.factor(data$cc_num)
data$zip <- as.factor(data$zip)
data$trans_date_trans_time = as_datetime(data$trans_date_trans_time)
data_bucket = data %>% group_by(cc_num) %>% summarize('count' = n())
library(cluster)
silhoette_width = sapply(2:20,
FUN = function(x) pam(x=data_bucket$count, k=x)$silinfo$avg.width)
ggplot(data=data.frame(cluster = 2:20,silhoette_width), aes(x=cluster,y=silhoette_width))+
geom_line(col='steelblue',size=1.2)+
geom_point()+
scale_x_continuous(breaks=seq(2,20,1))
set.seed(617)
km_profile = kmeans(x=data_bucket$count, centers=7, iter.max=10000, nstart=25)
k_segments_profile = km_profile$cluster
table(k_segments_profile)
data_bucket = cbind(data_bucket, 'profile' = k_segments_profile)
cc_representative = c(
(data_bucket %>% arrange(desc(count)) %>% filter(profile == 1))[1,1],
(data_bucket %>% arrange(desc(count)) %>% filter(profile == 2))[2,1],
(data_bucket %>% arrange(desc(count)) %>% filter(profile == 3))[3,1],
(data_bucket %>% arrange(desc(count)) %>% filter(profile == 4))[4,1],
(data_bucket %>% arrange(desc(count)) %>% filter(profile == 5))[5,1],
(data_bucket %>% arrange(desc(count)) %>% filter(profile == 6))[6,1],
(data_bucket %>% arrange(desc(count)) %>% filter(profile == 7))[7,1]
)
library(svDialogs)
#cc_number <- dlgInput("Enter credit card number", Sys.info()["user"])$res
cc_number = 30270432095985
# cc_number = as.numeric(as.character(cc_representative[1]))
# cc_number = as.numeric(as.character(cc_representative[2]))
# cc_number = as.numeric(as.character(cc_representative[3]))
# cc_number = as.numeric(as.character(cc_representative[4]))
# cc_number = as.numeric(as.character(cc_representative[5]))
# cc_number = as.numeric(as.character(cc_representative[6]))
# cc_number = as.numeric(as.character(cc_representative[7]))
data_indiv = filter(data, cc_num == cc_number)
within_ss = sapply(1:10,FUN = function(x){
set.seed(617)
kmeans(x=data_indiv$amt, centers=x, iter.max=1000, nstart=25)$tot.withinss})
ggplot(data=data.frame(cluster = 1:10,within_ss), aes(x=cluster,y=within_ss))+
geom_line(col='steelblue',size=1.2)+
geom_point()+
scale_x_continuous(breaks=seq(1,10,1))
ratio_ss = sapply(1:10,FUN = function(x) {
set.seed(617)
km = kmeans(x=data_indiv$amt, centers=x, iter.max=1000, nstart=25)
km$betweenss/km$totss} )
ggplot(data=data.frame(cluster = 1:10,ratio_ss), aes(x=cluster,y=ratio_ss))+
geom_line(col='steelblue',size=1.2)+
geom_point()+
scale_x_continuous(breaks=seq(1,10,1))
library(cluster)
silhoette_width = sapply(2:10,
FUN = function(x) pam(x=data_indiv$amt, k=x)$silinfo$avg.width)
ggplot(data=data.frame(cluster = 2:10,silhoette_width), aes(x=cluster,y=silhoette_width))+
geom_line(col='steelblue',size=1.2)+
geom_point()+
scale_x_continuous(breaks=seq(2,10,1))
d = dist(x = data_indiv$amt ,method = 'euclidean')
clusters = hclust(d = d,method='ward.D2')
h_segments = cutree(tree=clusters, k=9)
table(h_segments)
set.seed(617)
km = kmeans(x=data_indiv$amt, centers=9, iter.max=10000, nstart=25)
k_segments = km$cluster
table(k_segments)
library(mclust)
m_clusters = Mclust(data=data_indiv$amt)
m_segments = m_clusters$classification
#sort(table(m_segments))
#plot(m_clusters, what = "density", xlim = c(0, 300))
plot(m_clusters, what = "uncertainty", xlim = c(0, 500))
plot(m_clusters, what = 'classification', xlim = c(0, 500))
cat = data_indiv %>% select_if(is.factor) %>%  names()
cat_data = data_indiv[cat]
cat_data = cat_data[,c(3,13)]
library(klaR)
hasil = kmodes(cat_data, 7, iter.max = 7, weighted = FALSE, fast = TRUE)
kmode_segments = hasil$cluster
data_clusters = cbind(data_indiv, h_segments, k_segments, m_segments, kmode_segments)
data_clusters = data_clusters[,c(3,4,5,17,21:25)]
data_clusters$h_is_fraud_pred = 0
a = attributes(sort(table(h_segments))[1])$name
a = as.numeric(a)
b = attributes(sort(table(h_segments))[2])$name
b = as.numeric(b)
c = attributes(sort(table(h_segments))[3])$name
c = as.numeric(c)
d = attributes(sort(table(h_segments))[4])$name
d = as.numeric(d)
data_clusters$h_is_fraud_pred[data_clusters$h_segments == a] = 1
data_clusters$h_is_fraud_pred[data_clusters$h_segments == b] = 1
data_clusters$h_is_fraud_pred[data_clusters$h_segments == c] = 1
data_clusters$h_is_fraud_pred[data_clusters$h_segments == d] = 1
data_clusters$k_is_fraud_pred = 0
a = attributes(sort(table(k_segments))[1])$name
a = as.numeric(a)
b = attributes(sort(table(k_segments))[2])$name
b = as.numeric(b)
c = attributes(sort(table(k_segments))[3])$name
c = as.numeric(c)
d = attributes(sort(table(k_segments))[4])$name
d = as.numeric(d)
data_clusters$k_is_fraud_pred[data_clusters$k_segments == a] = 1
data_clusters$k_is_fraud_pred[data_clusters$k_segments == b] = 1
data_clusters$k_is_fraud_pred[data_clusters$k_segments == c] = 1
data_clusters$k_is_fraud_pred[data_clusters$k_segments == d] = 1
data_clusters$m_is_fraud_pred = 0
a = attributes(sort(table(m_segments))[1])$name
a = as.numeric(a)
b = attributes(sort(table(m_segments))[2])$name
b = as.numeric(b)
data_clusters$m_is_fraud_pred[data_clusters$m_segments == a] = 1
data_clusters$m_is_fraud_pred[data_clusters$m_segments == b] = 1
data_clusters$kmode_is_fraud_pred = 0
a = attributes(sort(table(kmode_segments))[1])$name
a = as.numeric(a)
b = attributes(sort(table(kmode_segments))[2])$name
b = as.numeric(b)
data_clusters$kmode_is_fraud_pred[data_clusters$kmode_segments == a] = 1
data_clusters$kmode_is_fraud_pred[data_clusters$kmode_segments == b] = 1
library(caret)
data_clusters$ensemble_pred = (data_clusters$m_is_fraud_pred | data_clusters$kmode_is_fraud_pred)
result_matrix = data_clusters[,c(4, 10:14)]
expected_value <- factor(result_matrix$is_fraud)
h_predicted_value <- factor(result_matrix$h_is_fraud_pred)
k_predicted_value <- factor(result_matrix$k_is_fraud_pred)
m_predicted_value <- factor(result_matrix$m_is_fraud_pred)
kmode_predicted_value <- factor(result_matrix$kmode_is_fraud_pred)
ensemble_predicted_value <- factor(result_matrix$ensemble_pred)
#Creating confusion matrix
h_cm <- confusionMatrix(data=h_predicted_value, reference = expected_value)
h_cm
k_cm <- confusionMatrix(data=k_predicted_value, reference = expected_value)
k_cm
m_cm <- confusionMatrix(data=m_predicted_value, reference = expected_value)
m_cm
kmode_cm <- confusionMatrix(data=kmode_predicted_value, reference = expected_value)
kmode_cm
ensemble_cm <- confusionMatrix(data=as.factor(as.numeric(ensemble_predicted_value)-1), reference = expected_value)
ensemble_cm
library(cvms)
d_binomial <- tibble("target" = result_matrix$is_fraud,
"prediction" = result_matrix$m_is_fraud_pred)
basic_table <- table(d_binomial)
cfm <- as_tibble(basic_table)
plot_confusion_matrix(cfm,
target_col = "target",
prediction_col = "prediction",
counts_col = "n")
ggmap(map)+
geom_point(data=data_spatial, aes(x=merch_long,y=merch_lat), size=1, alpha=0.2, color='seagreen')+
geom_point(data=data_spatial_fraud_pred, aes(x=merch_long,y=merch_lat), size=1, alpha=1, color='red')
library(caTools)
data_clusters2 = data_clusters[,c(2:5)]
set.seed(5205)
split = sample.split(data_clusters2$is_fraud, SplitRatio = 0.7)
train = data_clusters2[split,]
test = data_clusters2[!split,]
model = glm(is_fraud ~., data = train, family = 'binomial')
pred = predict(model, newdata = test, type = 'response')
expected_value2 <- factor(test$is_fraud)
glm_predicted_value <- factor(as.integer(pred>0.5))
glm_cm <- confusionMatrix(data=glm_predicted_value, reference = expected_value2)
glm_cm
data_map = filter(data, is_fraud == 1)
library(ggmap)
register_google(key = 'AIzaSyDoFcnGofCofZb2RvD5Bqwnv3buSWarFws')
map = get_map(location=c(-95.7129,37.0902), zoom=4, scale=4)
ggmap(map)+
geom_point(data=data_map, aes(x=merch_long,y=merch_lat), size=0.5, alpha=0.5, color='red')
data_map2 = data_map %>% group_by(state) %>% summarize('count' = n()) %>% arrange(desc(count))
data_map2
merch_lat = data_indiv$merch_lat
merch_long = data_indiv$merch_long
data_spatial = cbind(data_clusters, merch_lat, merch_long)
data_spatial_fraud_pred = filter(data_spatial, m_is_fraud_pred == 1)
data_spatial_fraud = filter(data_spatial, is_fraud == 1)
library(ggmap)
register_google(key = 'AIzaSyDoFcnGofCofZb2RvD5Bqwnv3buSWarFws')
map = get_map(location=c(median(data_spatial$merch_long), median(data_spatial$merch_lat)), zoom=8, scale=4)
ggmap(map)+
geom_point(data=data_spatial, aes(x=merch_long,y=merch_lat), size=1, alpha=0.2, color='seagreen')+
geom_point(data=data_spatial_fraud_pred, aes(x=merch_long,y=merch_lat), size=1, alpha=1, color='red')
ggmap(map)+
geom_point(data=data_spatial, aes(x=merch_long,y=merch_lat), size=1, alpha=0.2, color='seagreen')+
geom_point(data=data_spatial_fraud, aes(x=merch_long,y=merch_lat), size=1, alpha=1, color='red')
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/Woon/Desktop/Columbia/Applied Analytics/Term2/APAN5205/Tutorial")
data = read.csv(file = 'jpm_cluster.csv', stringsAsFactors = F)
names(data) = c('id','performance','fees_commissions','depth_of_products','ability_resolve_problems',
'online_services','choice_of_providers','quality_of_advice','knowledge_of_reps',
'rep_knowing_your_needs','professional_services','provider_knows_me',
'quality_of_service','age','marital_status','education')
data$age = factor(data$age,labels = c('27-57','58-68','69-75','75+'))
data$marital_status = factor(data$marital_status,labels=c('married','not married'))
data$education = factor(data$education,labels=c('no college','some college','college graduate',
'some graduate school','masters degree','doctorate'))
data_cluster = data[,2:13]
#total number of rows
nrow(data_cluster)
#checking for missing values
sum(is.na(data_cluster$performance))
#number of rows if the missing values were removed
nrow(na.omit(data_cluster))
# impute missing values
library(mice)
set.seed(617)
data_cluster = mice::complete(mice(data_cluster, use.matcher=T))
#scaling the variable is important so that the weight assigned to the variable is balanced
data_cluster = scale(data_cluster)
# compute similartiy measure based on distance. Using Euclidean distance here
d = dist(x = data_cluster, method = 'euclidean')
length(d)
# compute similarity measure based on distance. Using Euclidean distance here
d = dist(x = data_cluster, method = 'euclidean')
length(d)
#clustering methods
#Hierarchical clustering using Ward's method (There are other methods such as single, complete, linkage, median, centroid)
clusters = hclust(d = d, method = 'ward.D2')
plot(clusers)
plot(clusters)
#Goodness of Fit
# CPC>0.7 strong fit ::: 0.3<CPC<0.7 moderate fit
cor(cophenetic(clusters),d)
#Cutting trees to only display above a certain height
plot(cut(as.dendogram(clusters), h=5)$upper)
#Cutting trees to only display above a certain height
plot(cut(as.dendrogram(clusters), h=5)$upper)
rect.hclust(tree=clusters, k=3, border='tomato')
# compute similarity measure based on distance. Using Euclidean distance here
d = dist(x = data_cluster, method = 'euclidean')
length(d)
#clustering methods
#Hierarchical clustering using Ward's method (There are other methods such as single, complete, linkage, median, centroid)
clusters = hclust(d = d, method = 'ward.D2')
plot(clusters)
#height represents dissimilarity
#Goodness of Fit
# CPC>0.7 strong fit ::: 0.3<CPC<0.7 moderate fit
cor(cophenetic(clusters),d)
#Cutting trees to only display above a certain height
plot(cut(as.dendrogram(clusters), h=5)$upper)
#drawing rectangles around clusters
plot(clusters)
rect.hclust(tree=clusters, k=3, border='tomato')
#requires domain knowledge and some insights from the dendrogram
h_segments = cutree(tree = clusters, k=4)
h_segments
table(h_segments)
set.seed(617)
km = kmeans(x = data_cluster, centers = 3, iter.max=10000, nstart = 25)
k_segments = km$cluster
table(k_segments)
#ideal number of clusters is inferred from a sudden change in the line graph (elbow)
within_ss = sapply(1:10,FUN = function(x){
set.seed(617)
kmeans(x=data_cluster, centers=x, iter.max=1000, nstart=25)$tot.withinss})
ggplot(data=data.frame(cluster = 1:10,within_ss), aes(x=cluster,y=within_ss))+
geom_line(col='steelblue',size=1.2)+
geom_point()+
scale_x_continuous(breaks=seq(1,10,1))
#ideal number of clusters is inferred from a sudden change in the line graph (elbow)
library(ggplot2)
within_ss = sapply(1:10,FUN = function(x){
set.seed(617)
kmeans(x=data_cluster, centers=x, iter.max=1000, nstart=25)$tot.withinss})
ggplot(data=data.frame(cluster = 1:10,within_ss), aes(x=cluster,y=within_ss))+
geom_line(col='steelblue',size=1.2)+
geom_point()+
scale_x_continuous(breaks=seq(1,10,1))
#compute the ratio of between cluster sum of squares and total sum of squares
ratio_ss = sapply(1:10,FUN = function(x) {
set.seed(617)
km = kmeans(x=data_cluster, centers=x, iter.max=1000, nstart=25)
km$betweenss/km$totss} )
ggplot(data=data.frame(cluster = 1:10,ratio_ss), aes(x=cluster,y=ratio_ss))+
geom_line(col='steelblue',size=1.2)+
geom_point()+
scale_x_continuous(breaks=seq(1,10,1))
library(cluster)
silhoette_width = sapply(2:10,
FUN = function(x) pam(x=data_cluster, k=x)$silinfo$avg.width)
ggplot(data=data.frame(cluster = 2:10,silhoette_width), aes(x=cluster,y=silhoette_width))+
geom_line(col='steelblue',size=1.2)+
geom_point()+
scale_x_continuous(breaks=seq(2,10,1))
# 4 cluster solution is chosen to represent the niche
set.seed(617)
km = kmeans(x=data_cluster, centers=4, iter.max=10000, nstart=25)
# 4 cluster solution is chosen to represent the niche
set.seed(617)
km = kmeans(x=data_cluster, centers=4, iter.max=10000, nstart=25)
k_segments = km$cluster
table(k_segments)
library(mclust)
clusters_mclust = Mclust(data_cluster) #optimal cluster based on best BIC and log.likelihood
summary(clusters_mclust)
#can impose strict number of clusters
m_clusters = Mclust(data_cluster, G=4) #although 6 clusters provided the best BIC, may not be practical. Choose 4 since h and k also produced 4 clusters
m_segments = m_clusters$classification
table(m_segments)
table(m_segments)
data2 = cbind(data, h_segments, k_segments, m_segments)
knitr::opts_chunk$set(echo = TRUE)
setwd('C:/Users/Woon/Desktop/Columbia/Applied Analytics/Term2/APAN5205/FinalExam')
data_sec1 = read.csv('froyo.csv')
sum(is.na(data_sec1))
data_sec1_std = scale(data_sec1)
head(data_sec1_std)
#km 2 cent no max no nstart
set.seed(1031)
km = kmeans(x = data_sec1_std, centers = 2)
k_segments = km$cluster
table(k_segments)
km$betweenss/km$totss
set.seed(1031)
km3 = kmeans(x = data_sec1_std, centers = 3)
km3$betweenss/km3$totss
library(cluster)
library(ggplot2)
silhoette_width = sapply(2:4,
FUN = function(x) pam(x=data_sec1_std, k=x)$silinfo$avg.width)
ggplot(data=data.frame(cluster = 2:4,silhoette_width), aes(x=cluster,y=silhoette_width))+
geom_line(col='steelblue',size=1.2)+
geom_point()+
scale_x_continuous(breaks=seq(2,4,1))
mean(silhoette_width)
library(dplyr)
data_sec1_2 = as.data.frame(cbind(data_sec1_std, k_segments))
data_sec1_2 %>%
select(rating_PinkBerry:rating_16Handles, k_segments)%>%
group_by(k_segments)%>%
summarize_all(function(x) round(mean(x,na.rm=T),2))%>%
data.frame()
data_sec2 = read.csv('beauty.csv')
mean(nchar(data_sec2$review))
library(dplyr); library(tidytext)
data_temp = data_sec2%>%
select(id,review)%>%
unnest_tokens(output = word,input=review)%>%
group_by(id)%>%
summarize(count = n())%>%
ungroup()
median(data_temp$count)
data_sec2%>%
unnest_tokens(input = review, output = word)%>%
select(word)%>%
group_by(word)%>%
summarize(count = n())%>%
ungroup()%>%
arrange(desc(count))%>%
top_n(10)
data_sec2 %>%
select(id,review)%>%
unnest_tokens(output=word,input=review)%>%
inner_join(get_sentiments('bing'))%>%
group_by(sentiment)%>%
summarize(n = n())%>%
mutate(proportion = n/sum(n))%>%
ungroup()
data_sec2%>%
unnest_tokens(output = word, input = review)%>%
inner_join(get_sentiments('nrc'))%>%
group_by(sentiment)%>%
count()%>%
ungroup()
afinn = get_sentiments('afinn')
data_sec2%>%
select(id,review)%>%
unnest_tokens(output=word,input=review)%>%
inner_join(afinn)%>%
summarize(reviewSentiment = mean(value))
library(arules); library(arulesViz)
#items = read.transactions('shopping_data.rds',format='basket',sep=',')
items = readRDS('shopping_data.rds')
#items = read.transactions('shopping_data.rds', format = 'basket', sep=',')
#as(items,'matrix')
dim(items)[1]
#as(items, 'matrix')
dim(items)[2]
rules_all = apriori(items,parameter = list(support = 0.02, confidence = 0.02))
summary(rules_all)
#items
rules_2items = apriori(items,parameter=list(support=0.02,confidence=0.02,minlen=2,maxlen=2))
summary(rules_2items)
root_veg = subset(rules_2items,subset=lhs %pin% 'root vegetables')
inspect(root_veg)
summary(root_veg)
amaz = readRDS(file='amazon.RDS')
train = window(amaz,end=c(2017,12))
test = window(amaz, start=c(2018,01))
length(test)
library(ggplot2);library(ggthemes);library(gridExtra)  # For plots
library(quantmod);library(xts);library(zoo) # For using xts class objects
library(forecast) # Set of forecasting functions
library(fpp); library(fpp2) # Datasets from Forecasting text by Rob Hyndman
library(tseries) # for a statistical test
library(dplyr) # Data wrangling
naive_model = naive(train, h=48)
naive_model$mean
accuracy(naive_model)
accuracy(naive_model, x=amaz)
auto_arima_model = auto.arima(y = train, stepwise = F,approximation = F)
auto_arima_model
auto_arima_model_forecast = forecast(auto_arima_model,h=48)
auto_arima_model_forecast
#accuracy(f = auto_arima_model_forecast,x = amaz)
accuracy(auto_arima_model_forecast)
data_sec1_std = select(data_sec1, quality, variety, price, distance, courteousness, atmosphere)
head(data_sec1_std)
data_sec1_std = scale(data_sec1_std)
head(data_sec1_std)
#km 2 cent no max no nstart
set.seed(1031)
km = kmeans(x = data_sec1_std, centers = 2)
k_segments = km$cluster
table(k_segments)
km$betweenss/km$totss
set.seed(1031)
km3 = kmeans(x = data_sec1_std, centers = 3)
km3$betweenss/km3$totss
library(cluster)
library(ggplot2)
silhoette_width = sapply(2:4,
FUN = function(x) pam(x=data_sec1_std, k=x)$silinfo$avg.width)
ggplot(data=data.frame(cluster = 2:4,silhoette_width), aes(x=cluster,y=silhoette_width))+
geom_line(col='steelblue',size=1.2)+
geom_point()+
scale_x_continuous(breaks=seq(2,4,1))
mean(silhoette_width)
library(dplyr)
data_sec1_rate = select(data_sec1, rating_PinkBerry, rating_RedMango, rating_16Handles)
data_sec1_2 = as.data.frame(cbind(data_sec1_rate, k_segments))
data_sec1_2 %>%
select(rating_PinkBerry:rating_16Handles, k_segments)%>%
group_by(k_segments)%>%
summarize_all(function(x) round(mean(x,na.rm=T),2))%>%
data.frame()
library(arules); library(arulesViz)
items = readRDS('shopping_data.rds')
dim(items)[1]
#as(items, 'matrix')
dim(items)
items
rules_all = apriori(items,parameter = list(support = 0.02, confidence = 0.02))
summary(rules_all)
#items
inspect(rules_all)
rules41 = inspect(rules_all)
arrange(rules41, desc(lift))
rules41 = as.data.frame(inspect(rules_all))
arrange(rules41, desc(lift))
arrange(rules41)
arrange(rules41$lift)
max(rules41$lift)
rules_all = apriori(items,parameter = list(support = 0.02, confidence = 0.02))
summary(rules_all)
#items
inspect(root_veg)
summary(root_veg)
naive_model$mean
accuracy(auto_arima_model_forecast, x=amaz)
install.packages("Irkernel")
install.packages("IRkernel")
IRkernel::installspec()
shiny::runApp('C:/Users/Woon/Desktop/Columbia/Applied Analytics/Term3/6. SMASH/Ver 0.1')
runApp()
runApp('C:/Users/Woon/Desktop/Columbia/Applied Analytics/Term3/6. SMASH/Ver 0.1')
runApp('C:/Users/Woon/Desktop/Columbia/Applied Analytics/Term3/6. SMASH/Ver 0.1')
shiny::runApp('C:/Users/Woon/Desktop/Columbia/Applied Analytics/Term3/6. SMASH/Ver 0.1')
shiny::runApp('C:/Users/Woon/Desktop/Columbia/Applied Analytics/Term3/6. SMASH/Ver 0.1')
